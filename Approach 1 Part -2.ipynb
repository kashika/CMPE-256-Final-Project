{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load saved file as pandas dataframe\n",
    "df = pd.read_csv('yelp_dataset\\\\review_classification.csv', encoding = \"utf-8\", usecols = ['stars','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Very busy and noisy restaurant.\\r\\nAsparagas w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>On yelp 5 stars = Woohoo! as good as it gets! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>A great culinary experience from start to fini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Had the steak salad. It was amazing. Also had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Had Dinner at Delmonico on 11-5-15.  Still the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   stars                                               text\n",
       "0    2.0  Very busy and noisy restaurant.\\r\\nAsparagas w...\n",
       "1    5.0  On yelp 5 stars = Woohoo! as good as it gets! ...\n",
       "2    5.0  A great culinary experience from start to fini...\n",
       "3    5.0  Had the steak salad. It was amazing. Also had ...\n",
       "4    5.0  Had Dinner at Delmonico on 11-5-15.  Still the..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data details analysis\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 709155 entries, 0 to 709154\n",
      "Data columns (total 2 columns):\n",
      "stars    709155 non-null float64\n",
      "text     709155 non-null object\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 10.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         stars                                               text\n",
       "0         2.0  Very busy and noisy restaurant.\\r\\nAsparagas w...\n",
       "1         5.0  On yelp 5 stars = Woohoo! as good as it gets! ...\n",
       "2         5.0  A great culinary experience from start to fini...\n",
       "3         5.0  Had the steak salad. It was amazing. Also had ...\n",
       "4         5.0  Had Dinner at Delmonico on 11-5-15.  Still the...\n",
       "5         3.0  Disappointed. Steak overdone while waiter have...\n",
       "6         2.0  Rib eye steak for $51 should be pretty darn go...\n",
       "7         5.0  Amazing restaurant. Lets start off by saying r...\n",
       "8         4.0  4 Star for the whole experience and 5 stars fo...\n",
       "9         5.0  The meal started with Ryan greeting us and tak...\n",
       "10        5.0  This place is first class in every way. Lobste...\n",
       "11        5.0  This place is amazing!!!! Came here for an ann...\n",
       "12        1.0  Went there for my anniversary.  Knowing that t...\n",
       "13        3.0  5 stars for our waitress. Her service was impe...\n",
       "14        4.0  We have been here many times and have had an i...\n",
       "15        5.0  By far the best restaurant on the Las Vegas st...\n",
       "16        4.0  The decor is plain but that's not why you come...\n",
       "17        5.0  Exquisite in every sense of the word!\\r\\n\\r\\nG...\n",
       "18        5.0  Amazing service, even better food, I can't ima...\n",
       "19        5.0  Excellent food and service!! Everyone enjoyed ...\n",
       "20        5.0  This is mine and my fiancé's favorite steakhou...\n",
       "21        5.0  I wouldn't call myself fancy. I wouldn't call ...\n",
       "22        1.0  Wow...was super disappointed by my dinner here...\n",
       "23        4.0  Well?  I stay at the Palazzo or Venetian a few...\n",
       "24        5.0  We had a great experience!  Super friendly and...\n",
       "25        3.0  Very average mediocre steakhouse. The ambiance...\n",
       "26        5.0  Be sure to try as many appetizers as you can. ...\n",
       "27        5.0  Last night I had the salmon with a side of gri...\n",
       "28        5.0  This restaurant is amazing! We started with th...\n",
       "29        3.0  3.5 stars - above average steakhouse, amazing ...\n",
       "...       ...                                                ...\n",
       "709125    5.0  Wow. What an interesting idea. Ice cream cone ...\n",
       "709126    5.0  My girlfriend wanted have some desserts after ...\n",
       "709127    5.0  This is hands down my favorite crepe place! So...\n",
       "709128    5.0  My boys and I were looking for a late night de...\n",
       "709129    4.0  LOVE, LOVE, LOVEEEE this place! Great flavors ...\n",
       "709130    3.0  It's a decent crepe place.  A place to go to o...\n",
       "709131    2.0  I rated this place a 2 star because I thought ...\n",
       "709132    4.0  My first time trying a non traditional crepe a...\n",
       "709133    5.0  This place is right next to cafe darak and jus...\n",
       "709134    5.0  The owner is very friendly and the Crepes that...\n",
       "709135    4.0  I feel pretty down about it but I have to give...\n",
       "709136    4.0  I love their crepes. It's so fresh and soft an...\n",
       "709137    4.0  This place hits the sweet spot just right. I o...\n",
       "709138    5.0  I love coming here, the decor is so modern and...\n",
       "709139    5.0  The sweetness of the drinks are perfect. And t...\n",
       "709140    5.0  Are you craving one of those Asian style crepe...\n",
       "709141    5.0  The crepe is excellent. Not that sweet. Really...\n",
       "709142    5.0  The owner is very sweet! \\r\\n\\r\\nwhat's Crepe ...\n",
       "709143    4.0  First time here. This place is fairly new. Guy...\n",
       "709144    5.0  I came in yesterday for my first visit and wow...\n",
       "709145    4.0  em.....little upset because it doesn't come wi...\n",
       "709146    5.0  This has been my favorite place to visit so fa...\n",
       "709147    5.0  Ahh what an absolute blessing to this side of ...\n",
       "709148    3.0  A new crepe spot that just opened right on Dur...\n",
       "709149    4.0  This place was so yummy! I wanted something sw...\n",
       "709150    2.0  Came here for the first time on a Friday night...\n",
       "709151    5.0  Cute crepe spot! I like the variety of flavors...\n",
       "709152    5.0  Best Creme brûlée crepe in Las Vegas and maybe...\n",
       "709153    5.0  Really freakin good. I went twice in one week....\n",
       "709154    5.0  My favorite crepes are from Jean Philippe at t...\n",
       "\n",
       "[709155 rows x 2 columns]>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this project and opinion mining, we have set stars that higher than 4 to be positive, else negative\n",
    "df['target'] = df['stars'] > 4\n",
    "target_value = df['target'].values\n",
    "#print the target values\n",
    "target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.47785321967693944, 0.4995092793145308, (709155,))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# understand data pattern using mean and std\n",
    "target_value.mean(), target_value.std(), target_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Very busy and noisy restaurant.\\r\\nAsparagas was cooked perfectly, however quite flavorless. The mashed potatoes were tasty.  \\r\\nFor the price, the spinach should have been fresh and the cream sauce needs improvement. \\r\\nMy organic filet was good and nicely cooked to medium rare, however not near as tasty as other organic beef I have had for half the price.\\r\\nThe New Orleans gumbo was a tad too salty.  The yorkshire style buns were average and were cold.  \\r\\nThe key lime pie was average.  The tartness was lacking.  The apple pie was a disappointment, with a doughy flavoured crust.\\r\\nAnother thing that  high end restaurants need to learn is how to choose great coffees like good wines.  I asked where the beans were from and they had no idea.  I would expect excellence in all areas of my food consumption and yes, even with my coffee. The espresso was extremely poor.\\r\\nThis is a restaurant that is way overpriced for average to good tasting food.  Nothing was outstanding or excellent.\\r\\nSo I can only give it a rating of 2 combining value and flavour.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the reviews of dataframs\n",
    "reviews = df['text'].values\n",
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kashi\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# We have split the data into 80-20 split\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(reviews, target_value, test_size = 0.2, random_state = 66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use TFIDF for feature extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vec = TfidfVectorizer(stop_words = 'english', max_features = 350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(567324, 350)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Training and fitiing\n",
    "x_train_ = vec.fit_transform(x_train).toarray()\n",
    "x_train_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(141831, 350)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bag of words of tfidf\n",
    "bow = vec.get_feature_names()\n",
    "#fitting ntest data\n",
    "x_test_ = vec.fit_transform(x_test).toarray()\n",
    "x_test_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Similarity search in the corpus\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"We didn't eat here for fear of not finishing our food and having to get spanked lol but we did order the giant jello syringe shots which were good and strong! Wearing the hospital gown was fun and it was fun watching other people receive their punishment for not finishing their meals haha.\"]\n"
     ]
    }
   ],
   "source": [
    "rand_no = 66\n",
    "search_rev = x_test[rand_no]\n",
    "search_revs = [search_rev]\n",
    "#print randon reviews\n",
    "print(search_revs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to array\n",
    "vector_ = vec.transform(search_revs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check similarity scores\n",
    "sim_scores = cosine_similarity(vector_, x_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15609274, 0.        , 0.        , ..., 0.03885151, 0.        ,\n",
       "        0.01914538]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for top 5 similar reviews\n",
    "n = 5\n",
    "#sort the array based on similarity scores and return the top 5\n",
    "reviews = [x_train[i] for i in np.argsort(sim_scores[0])[::-1][:n]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review:\n",
      "We didn't eat here for fear of not finishing our food and having to get spanked lol but we did order the giant jello syringe shots which were good and strong! Wearing the hospital gown was fun and it was fun watching other people receive their punishment for not finishing their meals haha.\n",
      "\n",
      "Top 5 most similar reviews are displayed below:\n",
      "Review 0:\n",
      "\n",
      " the garlic knots are off the hook, so fresh and light...perfect amount of garlic and butter...a must try \n",
      "\n",
      "Review 1:\n",
      "\n",
      " Food was good! But when i ordered my food ( oxtail soup ) which is great! I asked my server if i could have some garlic on the side with it! Guess what??? The cafe dont have garlic! Are you kidding me? A restaurant with NO GARLIC! Lol... sooo if you are alergic or dont like garlic, this is the place for you! \n",
      "\n",
      "Review 2:\n",
      "\n",
      " Yum a ding a ding dong!! We had the triple garlic, Carolina gold,  and curry! Soooo good!! \n",
      "\n",
      "Review 3:\n",
      "\n",
      " Last night I ordered a pizza with garlic as one of the toppings. I expected some minced garlic to be sprinkled on as usual. Instead what I got was big chunks of garlic EVERYWHERE. There were layers of garlic, so much garlic you couldn't see the cheese or sauce under. I tried scraping pieces off and discovered more layers of garlic under the cheese. Because the chunks of garlic were so big, I couldn't even take a bite without crunching through lots of garlic, yuck! They absolutely RUINED my pizza, I'm so disappointed. Maybe THINK about the ingredient before covering pizza with it. Garlic is strong, who wants to taste only garlic?? There was more garlic than my other toppings. Absolutely disgusting. Way to ruin a pizza guys! Its a shame because I literally live up the street but I won't be back. Screw this location! \n",
      "\n",
      "Review 4:\n",
      "\n",
      " Only if you love garlic! Everything and I mean everything here is loaded with garlic. If you like garlic great if you're on a date lol or not a huge fan ask them a head of time how to order wo garlic in you food! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the top 5 most similar reviews\n",
    "#print review\n",
    "print('Review:')\n",
    "print(search_rev)\n",
    "\n",
    "#print similar reviews\n",
    "print('\\nTop %s most similar reviews are displayed below:' % n)\n",
    "for i, j in enumerate(reviews):\n",
    "    print('Review %s:' % i)\n",
    "    print('\\n',j,'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classifying the reviews as positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1 - baseline approach using Naive bayes\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the model\n",
    "nb = GaussianNB()\n",
    "nb.fit(x_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7530088626604903"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for training set with guassian NB\n",
    "nb.score(x_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6589109574070549"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for test set using guassion NB\n",
    "nb.score(x_test_, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To fine tune the parameters, we have used grid search CV\n",
    "#import relevant libraries\n",
    "#import metrices - classification report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impoprt logistic regression model\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter tuning values\n",
    "param_grid = [{'penalty':['l1'], 'C':[0.1, 1, 10]},\n",
    "              {'penalty':['l2'], 'C':[0.1, 1, 10]}]\n",
    "\n",
    "accuracy = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the hyper-parameters for accuracy\n",
      "\n",
      "Best parameters found on training set:\n",
      "\n",
      "{'C': 1, 'penalty': 'l2'}\n",
      "\n",
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.63      0.83      0.72     73967\n",
      "       True       0.72      0.47      0.57     67864\n",
      "\n",
      "avg / total       0.68      0.66      0.65    141831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in accuracy:\n",
    "    print(\"Fine tuning the hyper-parameters for %s\" % i + \"\\n\")\n",
    "    #pass tuning parameters\n",
    "    model = GridSearchCV(LogisticRegression(), param_grid, cv=5,scoring=i)\n",
    "    #fit the model\n",
    "    model.fit(x_train_[:500,:], y_train[:500])\n",
    "    print(\"Best parameters found on training set:\\n\")\n",
    "    print(model.best_params_)\n",
    "    #print classification report\n",
    "    print(\"\\nClassification report:\\n\")\n",
    "    actual, predicted = y_test, model.predict(x_test_)\n",
    "    print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit logistic regression model with fine tuned parameters\n",
    "lr = LogisticRegression(C =1 ,penalty = 'l2')\n",
    "lr.fit(x_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7990389971162862"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for training set using logistic regression\n",
    "lr.score(x_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7023006253921921"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for test set using logistic regression\n",
    "lr.score(x_test_, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'best',\n",
       " 'thank',\n",
       " 'awesome',\n",
       " 'delicious',\n",
       " 'perfect',\n",
       " 'highly',\n",
       " 'fantastic',\n",
       " 'excellent',\n",
       " 'favorite',\n",
       " 'great',\n",
       " 'wonderful',\n",
       " 'love',\n",
       " 'perfectly',\n",
       " 'loved',\n",
       " 'definitely',\n",
       " 'vegas',\n",
       " 'happy',\n",
       " 'absolutely',\n",
       " 'chef']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 20 features or words that make positive prediction\n",
    "n = 20\n",
    "[bow[i] for i in np.argsort(lr.coef_[0])[::-1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['worst',\n",
       " 'horrible',\n",
       " 'rude',\n",
       " 'ok',\n",
       " 'okay',\n",
       " 'slow',\n",
       " 'dry',\n",
       " 'decent',\n",
       " 'wasn',\n",
       " 'reason',\n",
       " 'wouldn',\n",
       " 'overall',\n",
       " 'bad',\n",
       " 'maybe',\n",
       " 'money',\n",
       " 'cold',\n",
       " 'pretty',\n",
       " 'didn',\n",
       " 'used',\n",
       " 'told']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 20 features or words that make negative prediction\n",
    "[bow[i] for i in np.argsort(lr.coef_[0])[:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune the hyper-parameters\n",
    "param_grid = [{'n_estimators':[5, 10,15,20], 'min_samples_leaf':[1, 3, 5, 7]},\n",
    "              {'n_estimators':[5, 10,15,20], 'min_samples_leaf':[1, 3, 5, 7]}]\n",
    "\n",
    "#scoring metrics\n",
    "accuracy = ['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning the hyper-parameters for accuracy\n",
      "\n",
      "Best parameters found on training set:\n",
      "\n",
      "{'n_estimators': 15, 'min_samples_leaf': 5}\n",
      "\n",
      "Classification report:\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "      False       0.63      0.79      0.70     73967\n",
      "       True       0.68      0.49      0.57     67864\n",
      "\n",
      "avg / total       0.65      0.64      0.64    141831\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in accuracy:\n",
    "    print(\"Fine tuning the hyper-parameters for %s\" % i + \"\\n\")\n",
    "    model = GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring=i)\n",
    "    #fitting the model\n",
    "    model.fit(x_train_[:500,:], y_train[:500])\n",
    "    print(\"Best parameters found on training set:\\n\")\n",
    "    print(model.best_params_)\n",
    "    #print classification report\n",
    "    print(\"\\nClassification report:\\n\")\n",
    "    actual, predicted = y_test, model.predict(x_test_)\n",
    "    print(classification_report(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=15, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(max_depth = None,n_estimators = 15, min_samples_leaf = 1)\n",
    "rf.fit(x_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9943559588524371"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for training set using random forests\n",
    "rf.score(x_train_, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6867257510699354"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy score for test set using random forests\n",
    "rf.score(x_test_, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazing',\n",
       " 'great',\n",
       " 'best',\n",
       " 'delicious',\n",
       " 'love',\n",
       " 'awesome',\n",
       " 'good',\n",
       " 'ok',\n",
       " 'vegas',\n",
       " 'definitely',\n",
       " 'didn',\n",
       " 'food',\n",
       " 'bad',\n",
       " 'like',\n",
       " 'excellent',\n",
       " 'favorite',\n",
       " 'place',\n",
       " 'asked',\n",
       " 'friendly',\n",
       " 'service']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the top 20 most important features/words\n",
    "n = 20\n",
    "[bow[i] for i in np.argsort(rf.feature_importances_)[::-1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries for cross validation score\n",
    "#import time to check execution time for different models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution Time for Navie Bayes -:  52.23256754875183  Accuracy:  0.7528519865428325\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# naive bayes cross validation \n",
    "cv_ = cross_val_score(nb,x_train_,y_train,cv = 5, scoring=\"accuracy\")\n",
    "exec_time = time.time() - start_time\n",
    "#print execution time\n",
    "print(\"Execution Time for Navie Bayes -: \",exec_time,\" Accuracy: \",np.mean(cv_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for Logistic Regression -:  48.92613506317139  Accuracy:  0.7985718919218331\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# logistic regression execution time\n",
    "cv_ = cross_val_score(lr,x_train_, y_train, cv = 5,scoring=\"accuracy\")\n",
    "exec_time = time.time() - start_time\n",
    "#print execution time\n",
    "print(\"Execution time for Logistic Regression -: \",exec_time,\" Accuracy: \",np.mean(cv_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time for Random Forest -:  48.92613506317139  Accuracy:  0.778463449191958\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# random forest execution time\n",
    "cv_ = cross_val_score(rf,x_train_, y_train, cv = 5, scoring=\"accuracy\")\n",
    "exe_time = time.time() - start_time\n",
    "#print execution time\n",
    "print(\"Execution time for Random Forest -: \",exec_time,\" Accuracy: \",np.mean(cv_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the different models we can see that the accuracy is best for Logistic regression. Random forests accuracy is less comparitively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
